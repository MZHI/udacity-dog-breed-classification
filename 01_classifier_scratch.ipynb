{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and train dog breed classificator from scratch\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, the process of implementation, training and improvement the neural network from scratch is presented. We go step-by-step from basic AlexNet initinal model to get accuracy on test dataset at least 10%.\n",
    "\n",
    "The notebook is broken into separate steps. Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Basic model realization and description\n",
    "* [Step 2](#step2): Experiments and results \n",
    "* [Step 3](#step3): \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "Make sure that you've downloaded the required human and dog datasets:\n",
    "* Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in this project's home directory, at the location `/dogImages`. \n",
    "\n",
    "*Note: If you are using a Windows machine, you are encouraged to use [7zip](http://www.7-zip.org/) to extract the folder.*\n",
    "\n",
    "In the code cell below, we save the file paths for the dog dataset in the numpy arrays `dog_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8351 total dog images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load filenames for dog images\n",
    "dog_files = np.array(glob(\"data/dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Basic Model Architecture\n",
    "\n",
    "Specify Data Loaders for the Dog Dataset\n",
    "\n",
    "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dog_images/train`, `dog_images/valid`, and `dog_images/test`, respectively).  You may find [this documentation on custom datasets](http://pytorch.org/docs/stable/torchvision/datasets.html) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA available: {}\".format(use_cuda))\n",
    "\n",
    "### Data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(256),\n",
    "   transforms.CenterCrop(224),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"data/dogImages/train/\", transform=preprocess)\n",
    "valid_data = datasets.ImageFolder(\"data/dogImages/valid/\", transform=preprocess)\n",
    "test_data = datasets.ImageFolder(\"data/dogImages/test/\", transform=preprocess)\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "loaders_scratch = {}\n",
    "loaders_scratch['train'] = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "loaders_scratch['valid'] = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "loaders_scratch['test'] = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 133\n"
     ]
    }
   ],
   "source": [
    "# get number of classes\n",
    "NCLASSES = len(train_data.class_to_idx)\n",
    "print(\"Number of classes: {}\".format(NCLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: calculate mean and std of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4864, 0.4560, 0.3918])\n",
      "length of dataset: Dataset ImageFolder\n",
      "    Number of datapoints: 6680\n",
      "    Root location: data/dogImages/train/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "           )\n",
      "length of sampler: <torch.utils.data.sampler.SequentialSampler object at 0x7fe45ee7eb70>\n",
      "tensor([0.2602, 0.2536, 0.2562])\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(256),\n",
    "   transforms.CenterCrop(224),\n",
    "   transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = datasets.ImageFolder(\"data/dogImages/train/\", transform=preprocess)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                         batch_size=10,\n",
    "                         num_workers=0,\n",
    "                         shuffle=False)\n",
    "\n",
    "mean = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0) \n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "mean = mean / len(loader.dataset)\n",
    "print(mean)\n",
    "\n",
    "print(\"length of dataset: {}\".format(loader.dataset))\n",
    "print(\"length of sampler: {}\".format(loader.sampler))\n",
    "\n",
    "var = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "std = torch.sqrt(var / (len(loader.dataset)*224*224))\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN implementation\n",
    "\n",
    "Basic CNN model based on AlexNet architecture [paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) with some simplifications:\n",
    "- not used `Local Response Normalization`\n",
    "- using single GPU instead of how in paper (two GPUs)\n",
    "\n",
    "---\n",
    "Here, the vizualization of model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicCNN(\n",
      "  (conv1): Conv2d(3, 48, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (conv2): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4608, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (fc3): Linear(in_features=2048, out_features=133, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.nn_alex_net import BasicCNN\n",
    "\n",
    "model_scratch = BasicCNN(n_classes=NCLASSES)\n",
    "print(model_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Experiments\n",
    "\n",
    "Description of experiments.\n",
    "\n",
    "To speed up training process, all stuff code such train, test were moved to .py scripts, to run experiments in console mode and use several GPU's. Here just experiments details are described and corresponding results are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments description\n",
    "\n",
    "#### Experiment 0: use only initial model, without train\n",
    "##### **Results**:\n",
    "* stopped on epoch: --\n",
    "* train loss: --\n",
    "* val loss: --\n",
    "* test loss: TODO\n",
    "* test accuracy: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to get test accuracy\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1\n",
    "* checkpoint name: TODO \n",
    "* model: BasicCNN\n",
    "* optimezer: SGD\n",
    "* momentum: 0.9\n",
    "* lr: 0.01\n",
    "* weight decay: no\n",
    "* augmentations: no\n",
    "* batch size: 32\n",
    "\n",
    "##### **Results**:\n",
    "* stopped on epoch: TODO\n",
    "* train loss: TODO\n",
    "* val loss: TODO\n",
    "* test loss: TODO\n",
    "* test accuracy: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to get test accuracy\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
